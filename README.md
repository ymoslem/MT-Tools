# MT-Tools
Collection of Common Machine Translation Tools

## MT Frameworks

### Neural Machine Translation

* [FairSeq](https://github.com/pytorch/fairseq)
* [Marian](https://github.com/marian-nmt/marian)
* [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)
* [OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf)
* [Sockeye](https://github.com/awslabs/sockeye)

**Inference Engine**
* [CTranslate2](https://github.com/OpenNMT/CTranslate2)


### Statistical Machine Translation

* [Moses](http://www.statmt.org/moses/)


## Tokenization & Preprocessing

* [NLTK](https://www.nltk.org/)
* [SentencePiece](https://github.com/google/sentencepiece)
* [SacreMoses](https://github.com/alvations/sacremoses)
* [spaCy](https://spacy.io/)
* [Tokenizer](https://github.com/OpenNMT/Tokenizer)
* [YouTokenToMe](https://github.com/VKCOM/YouTokenToMe)


**Indic Languages NLP Tools**

* [iNLTK](https://github.com/goru001/inltk)
* [Indic NLP Library](http://anoopkunchukuttan.github.io/indic_nlp_library/)
* [indic-trans](https://github.com/libindic/indic-trans)


## MT Evaluation

* [Compare-MT](https://github.com/neulab/compare-mt): Multiple MT evaluation metics.
* [NLPMetrics](https://github.com/gcunhase/NLPMetrics):  Multiple MT evaluation metics.
* [SacreBLEU](https://github.com/mjpost/sacrebleu): MT evaluation metic, supporting BLEU, spBLEU, chrF++, TER, WER
* [WER](https://github.com/jitsi/jiwer): WER MT evaluation metic.
* [PyTER](https://pypi.org/project/pyter/0.2.2.1/): TER MT evaluation metic.
* [COMET](https://github.com/Unbabel/COMET): Semantic MT evaluation metic.
* [YISI](https://github.com/chikiulo/yisi): Semantic MT evaluation metic.

**Relevant Tutorials**

* [Computing BLEU Score for Machine Translation](https://blog.machinetranslation.io/compute-bleu-score/)
* [WER Score for Machine Translation](https://blog.machinetranslation.io/compute-wer-score/)


## General-Purpose ML/DL Frameworks

* [NumPy](https://github.com/numpy/numpy)
* [Pandas](https://github.com/pandas-dev/pandas)
* [scikit-learn](https://scikit-learn.org/stable/)
* [PyTorch](https://pytorch.org/)
* [TensorFlow](https://www.tensorflow.org/)
* [Keras](https://keras.io/)
* [Trax](https://github.com/google/trax)


## Bilingual Datasets

* [OPUS](http://opus.nlpl.eu/)


## Pre-trained NMT Models

Most of these models can be converted to the [CTranslate2](https://github.com/OpenNMT/CTranslate2) format for better efficiency. You can also run them out-of-the-box with [HuggingFace](https://huggingface.co/), a framework supporting pre-trained models for multiple NLP purposes.

* OPUS-MT ([Tatoeba-Challenge](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models)): Several MT models
* [M2M-100](https://github.com/facebookresearch/fairseq/tree/main/examples/m2m_100)
* [mBART](https://github.com/facebookresearch/fairseq/tree/main/examples/mbart)
* [NLLB](https://github.com/facebookresearch/fairseq/tree/nllb)
* [T5](https://github.com/google-research/text-to-text-transfer-transformer)

**Other pre-trained NLP models:**

* [BERT](https://github.com/google-research/bert)
* [GPT-2](https://github.com/openai/gpt-2)
* [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b)
* [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)
* [OPT](https://github.com/facebookresearch/metaseq)


## Local NMT Tools

* [ArgosTranslate](https://github.com/argosopentech/argos-translate)
* [DesktopTranslator](https://github.com/ymoslem/DesktopTranslator)
* [Local-NMT](https://github.com/fantinuoli/Local-NMT)
* [translateLocally](https://github.com/XapaJIaMnu/translateLocally)







